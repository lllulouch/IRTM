{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拍謝頗雜亂\n",
    "by pochun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5739\n",
      "5\n",
      "Index(['title', 'object_class', 'content', 'rating', 'url'], dtype='object')\n",
      "['euclid' 'thaumiel' 'keter' 'safe' 'neutralized']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/irtm.csv')  \n",
    "count_row = df.shape[0] \n",
    "count_col = df.shape[1]\n",
    "print(count_row)\n",
    "print(count_col)\n",
    "print(df.columns)\n",
    "print(df['object_class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rating \n",
      " title                                                     SCP-173\n",
      "object_class                                               euclid\n",
      "content         SCP-173 in containment\\n SCP-173\\n Euclid\\n It...\n",
      "rating                                                       6541\n",
      "url                           http://scp-wiki.wikidot.com/scp-173\n",
      "Name: 4354, dtype: object \n",
      "\n",
      "Min rating \n",
      " title                                                 SCP-138-ARC\n",
      "object_class                                               euclid\n",
      "content          SCP-138\\n Euclid\\n \\n \\n SCP-138 appears to b...\n",
      "rating                                                        -59\n",
      "url                       http://scp-wiki.wikidot.com/scp-138-arc\n",
      "Name: 5023, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Max rating \\n\" , df.loc[df['rating'].idxmax()] , \"\\n\")\n",
    "print(\"Min rating \\n\" , df.loc[df['rating'].idxmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.plot.hist(bins = 100 , xlim = (-100 , 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              title  content  rating   url\n",
      "object_class                              \n",
      "euclid         2368     2367    2368  2368\n",
      "keter          1024     1024    1024  1024\n",
      "neutralized     231      231     231   231\n",
      "safe           1987     1984    1987  1987\n",
      "thaumiel        129      129     129   129\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('object_class').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [i for i in range(count_row)]\n",
    "df['index'] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training validation testing 的切法還須調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[4500 : count_row] \n",
    "val_df = df.iloc[3000 : 4000]\n",
    "test_df = df.iloc[0 : 1000]\n",
    "train_df = train_df.reset_index()\n",
    "val_df = val_df.reset_index()\n",
    "test_df = test_df.reset_index()\n",
    "train_df = train_df.drop(columns = ['level_0'])\n",
    "val_df = val_df.drop(columns = ['level_0'])\n",
    "test_df = test_df.drop(columns = ['level_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_test_df = test_df.groupby(\"object_class\")\n",
    "euclid_test_df = class_test_df.get_group(\"euclid\")\n",
    "thaumiel_test_df = class_test_df.get_group(\"thaumiel\")\n",
    "keter_test_df = class_test_df.get_group(\"keter\")\n",
    "safe_test_df = class_test_df.get_group(\"safe\")\n",
    "# 照此切法 testing 沒有 neutralized QQ\n",
    "# neutralized_test_df = class_test_df.get_group(\"neutralized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train_df = train_df.groupby(\"object_class\")\n",
    "euclid_train_df = class_train_df.get_group(\"euclid\")\n",
    "thaumiel_train_df = class_train_df.get_group(\"thaumiel\")\n",
    "keter_train_df = class_train_df.get_group(\"keter\")\n",
    "safe_train_df = class_train_df.get_group(\"safe\")\n",
    "neutralized_train_df = class_train_df.get_group(\"neutralized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_index_list_train = euclid_train_df['index'].tolist()\n",
    "t_index_list_train = thaumiel_train_df['index'].tolist()\n",
    "k_index_list_train = keter_train_df['index'].tolist()\n",
    "s_index_list_train = safe_train_df['index'].tolist()\n",
    "n_index_list_train = neutralized_train_df['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是我的 PA3 經過些微調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def term_extraction (txt):\n",
    "    txt = str(txt) \n",
    "    punctuation = [\"\\n\" , \"``\" , '$' , ', ' , '. ' , '; ' , '.' , '_' , ':' , \"''\" , '\\\"' , '\\'' , '\\\"' , '(' , ')', '!' , '?' , '‘' , '*' , '&' , '`' , '\\\\']\n",
    "    for mark in punctuation:\n",
    "        txt = txt.replace(mark , ' ')\n",
    "    txt = txt.replace(',' , '')\n",
    "    txt = txt.replace(\"\\'s \" , \" is \")\n",
    "    txt = txt.replace(\"-\" , \" \")\n",
    "    split_txt = []\n",
    "    temp = ''\n",
    "    for char in txt:\n",
    "        if char == ' ':\n",
    "            split_txt.append(temp)\n",
    "            temp = ''\n",
    "        else:\n",
    "            temp += char\n",
    "    if temp:\n",
    "        split_txt.append(temp)\n",
    "    token = []\n",
    "    for s in split_txt:\n",
    "        token.append(s.lower())\n",
    "    ps_token = []\n",
    "    for word in token:\n",
    "        ps_token.append(ps.stem(word))\n",
    "    stop_word = [\"\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    final = []\n",
    "    for word in ps_token:\n",
    "        appear = False\n",
    "        for stop in stop_word:\n",
    "            if word == stop:\n",
    "                appear = True\n",
    "        if not appear and word.isalpha():\n",
    "            final.append(word)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dictionary (vocabulary_collection) :\n",
    "    dictionary = pd.DataFrame(columns = ['term' , 'df' , 'cf'])\n",
    "    df = 0\n",
    "    cf = 0\n",
    "    for d in vocabulary_collection:\n",
    "        for t in d:\n",
    "            if t not in dictionary.values:        \n",
    "                for d2 in vocabulary_collection :\n",
    "                    cf = cf + d2.count(t)\n",
    "                    if t in d2:\n",
    "                        df = df + 1\n",
    "                dictionary.loc[-1] = [t , df , cf]  \n",
    "                dictionary.index = dictionary.index + 1\n",
    "            df = 0\n",
    "            cf = 0\n",
    "    dictionary.sort_values(by = ['term'] , inplace = True , ascending = True)\n",
    "    dictionary = dictionary.reset_index(drop = True)\n",
    "    dictionary.to_csv('pochun_testing/dictionary.txt' , index = None , sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class_dictionary (train_df , dictionary) :\n",
    "    class_df = []\n",
    "    for i in range (5) :\n",
    "        if (i == 0) :\n",
    "            cdocument_list = e_index_list_train\n",
    "        elif (i == 1) :\n",
    "            cdocument_list = t_index_list_train\n",
    "        elif (i == 2) :\n",
    "            cdocument_list = k_index_list_train\n",
    "        elif (i == 3) :\n",
    "            cdocument_list = s_index_list_train\n",
    "        elif (i == 4) :\n",
    "            cdocument_list = n_index_list_train\n",
    "        else :\n",
    "            print(\"wrong_class_index\")\n",
    "\n",
    "        class_collection = ''\n",
    "        for docId in cdocument_list :     \n",
    "            class_df_index = train_df[train_df['index'] == docId].index.tolist()\n",
    "            index = class_df_index[0]\n",
    "            string = train_df.iloc[index]['content']  \n",
    "            class_collection = class_collection + string\n",
    "        class_vocabulary = term_extraction(class_collection)\n",
    "        c = pd.DataFrame(columns = ['term' , 'classf'])\n",
    "        for t in dictionary['term'] :\n",
    "            classf = class_vocabulary.count(t)\n",
    "            c.loc[-1] = [t , classf]\n",
    "            c.index = c.index + 1    \n",
    "        class_df.append(c)\n",
    "    for c in class_df :\n",
    "        prob = []\n",
    "        for index , row in c.iterrows() :        \n",
    "            prob.append(math.log((row['classf'] + 1) / (c['classf'].sum() + 500)))\n",
    "        c['probability'] = prob\n",
    "    return class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class_vocabulary (train_df) :\n",
    "    class_vocabulary = [] \n",
    "    \n",
    "    for i in range(5):\n",
    "        class_vocabulary.append([])\n",
    "\n",
    "    for i in range (5) :\n",
    "        if (i == 0) :\n",
    "            cdocument_list = e_index_list_train\n",
    "        elif (i == 1) :\n",
    "            cdocument_list = t_index_list_train\n",
    "        elif (i == 2) :\n",
    "            cdocument_list = k_index_list_train\n",
    "        elif (i == 3) :\n",
    "            cdocument_list = s_index_list_train\n",
    "        elif (i == 4) :\n",
    "            cdocument_list = n_index_list_train\n",
    "        else :\n",
    "            print(\"wrong_class_index\")\n",
    "        \n",
    "        for docId in cdocument_list :\n",
    "#             print(docId)\n",
    "            class_df_index = train_df[train_df['index'] == docId].index.tolist()\n",
    "#             print(class_df_index)\n",
    "            index = class_df_index[0]\n",
    "            string = train_df.iloc[index]['content'] \n",
    "            class_vocabulary[i].append(term_extraction(string)) \n",
    "    return class_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_test_df (test_vocabulary_collection , dictionary) :\n",
    "    test_df = []\n",
    "    for d in test_vocabulary_collection :\n",
    "        tdoc = pd.DataFrame(columns = ['term' , 'tf'])\n",
    "        for t in d :\n",
    "            tf = 0\n",
    "            if (t in dictionary.values) and (t not in tdoc.values) :\n",
    "                tf = d.count(t)\n",
    "                tdoc.loc[-1] = [t , tf]\n",
    "                tdoc.index = tdoc.index + 1    \n",
    "        test_df.append(tdoc)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(\"pochun_testing\")\n",
    "all_list = []\n",
    "for i in range(count_row) :\n",
    "    all_list.append(str(i))\n",
    "train_list = [i for i in range(4500 , count_row)]\n",
    "val_list = [i for i in range(3000 , 4000)]\n",
    "test_list = [i for i in range(0 , 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "for docId in train_list : \n",
    "    class_df_index = df[df['index'] == docId].index.tolist()\n",
    "    index = class_df_index[0]\n",
    "    string = df.iloc[index]['content'] \n",
    "    collection.append(string)\n",
    "vocabulary_collection = []\n",
    "for i in range(len(collection)):\n",
    "    vocabulary_collection.append(term_extraction(collection[i]))\n",
    "to_dictionary(vocabulary_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pochunwu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\pochunwu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\pochunwu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "C:\\Users\\pochunwu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\pochunwu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\pochunwu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "dictionary = pd.read_csv('pochun_testing/dictionary.txt' , sep = \" \")\n",
    "zeros = [0] * len(dictionary)\n",
    "dictionary['chi'] = zeros\n",
    "class_vocabulary = to_class_vocabulary(train_df)\n",
    "chi_table = []\n",
    "\n",
    "for term_index in range(len(dictionary)) :\n",
    "    chi_n_2 = pd.DataFrame()\n",
    "    zeros13 = [-1] * 13\n",
    "    chi_n_2['present'] = zeros13\n",
    "    chi_n_2['absent'] = zeros13\n",
    "    term = dictionary.iloc[term_index]['term']\n",
    "    for class_index in range(len(class_vocabulary)) :\n",
    "        present = 0\n",
    "        for class_doc_index in range(len(class_vocabulary[class_index])) :\n",
    "            \n",
    "            if term in class_vocabulary[class_index][class_doc_index] :\n",
    "                present = present + 1\n",
    "        chi_n_2['present'][class_index] = present\n",
    "        absent = 15 - present\n",
    "        chi_n_2['absent'][class_index] = absent\n",
    "    chi_score = 0\n",
    "    for index , row in chi_n_2.iterrows() :\n",
    "        expect = (15 / (13 * 15)) * (chi_n_2['present'].sum() / (13 * 15)) * (13 * 15)\n",
    "        n = row['present']\n",
    "        chi_score = chi_score + ((n - expect)**2 / expect)\n",
    "        expect = (15 / (13 * 15)) * (chi_n_2['absent'].sum() / (13 * 15)) * (13 * 15)\n",
    "        n = row['absent']\n",
    "        chi_score = chi_score + ((n - expect)**2 / expect)\n",
    "    dictionary['chi'][term_index] = chi_score\n",
    "dictionary = dictionary.sort_values(by = ['chi'] , ascending = False)\n",
    "dictionary = dictionary.sort_values(by = ['chi'] , ascending = False)\n",
    "dictionary = dictionary.reset_index(drop = True)\n",
    "dictionary = dictionary.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chi square == inf 應該有錯誤QQ(暫時查無原因QQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          term  df  cf         chi\n",
      "0     restrain  39  50         inf\n",
      "1      nostril   3   3         inf\n",
      "2        press  14  17         inf\n",
      "3    hemispher   1   1         inf\n",
      "4        tumbl   2   5         inf\n",
      "..         ...  ..  ..         ...\n",
      "495   facsimil   1   1  489.689655\n",
      "496        fee   2   2  489.689655\n",
      "497    vocalis   3   3  489.689655\n",
      "498  physician   4   5  489.689655\n",
      "499    patienc   7   7  489.689655\n",
      "\n",
      "[500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = to_class_dictionary(train_df , dictionary)\n",
    "for i in range(len(class_df)) :\n",
    "    class_df[i] = class_df[i].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[          term classf  probability\n",
      "0     restrain     40    -4.832986\n",
      "1      nostril      2    -7.447946\n",
      "2        press     15    -5.773969\n",
      "3    hemispher      4    -6.937120\n",
      "4        tumbl      1    -7.853411\n",
      "..         ...    ...          ...\n",
      "495   facsimil      1    -7.853411\n",
      "496        fee      3    -7.160263\n",
      "497    vocalis      1    -7.853411\n",
      "498  physician      1    -7.853411\n",
      "499    patienc      1    -7.853411\n",
      "\n",
      "[500 rows x 3 columns],           term classf  probability\n",
      "0     restrain      4    -6.582025\n",
      "1      nostril      1    -7.498316\n",
      "2        press     26    -4.895626\n",
      "3    hemispher      5    -6.399704\n",
      "4        tumbl      1    -7.498316\n",
      "..         ...    ...          ...\n",
      "495   facsimil      2    -7.092851\n",
      "496        fee      3    -6.805169\n",
      "497    vocalis      3    -6.805169\n",
      "498  physician      4    -6.582025\n",
      "499    patienc      2    -7.092851\n",
      "\n",
      "[500 rows x 3 columns],           term classf  probability\n",
      "0     restrain     19    -5.480431\n",
      "1      nostril      3    -7.089868\n",
      "2        press     28    -5.108867\n",
      "3    hemispher      2    -7.377551\n",
      "4        tumbl      2    -7.377551\n",
      "..         ...    ...          ...\n",
      "495   facsimil      3    -7.089868\n",
      "496        fee      3    -7.089868\n",
      "497    vocalis      1    -7.783016\n",
      "498  physician      1    -7.783016\n",
      "499    patienc      3    -7.089868\n",
      "\n",
      "[500 rows x 3 columns],           term classf  probability\n",
      "0     restrain      9    -5.465525\n",
      "1      nostril      8    -5.570886\n",
      "2        press     11    -5.283204\n",
      "3    hemispher      2    -6.669498\n",
      "4        tumbl      1    -7.074963\n",
      "..         ...    ...          ...\n",
      "495   facsimil      3    -6.381816\n",
      "496        fee      2    -6.669498\n",
      "497    vocalis      7    -5.688669\n",
      "498  physician      3    -6.381816\n",
      "499    patienc      1    -7.074963\n",
      "\n",
      "[500 rows x 3 columns],           term classf  probability\n",
      "0     restrain     15    -5.704407\n",
      "1      nostril      1    -7.783849\n",
      "2        press     23    -5.298942\n",
      "3    hemispher      1    -7.783849\n",
      "4        tumbl      3    -7.090702\n",
      "..         ...    ...          ...\n",
      "495   facsimil      3    -7.090702\n",
      "496        fee      1    -7.783849\n",
      "497    vocalis      2    -7.378384\n",
      "498  physician      2    -7.378384\n",
      "499    patienc      2    -7.378384\n",
      "\n",
      "[500 rows x 3 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(class_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection = []\n",
    "for docId in test_list:        \n",
    "    class_df_index = test_df[test_df['index'] == docId].index.tolist()\n",
    "    index = class_df_index[0]\n",
    "    string = test_df.iloc[index]['content'] \n",
    "    test_collection.append(string)\n",
    "test_vocabulary_collection = []\n",
    "for i in range(len(test_collection)):\n",
    "    test_vocabulary_collection.append(term_extraction(test_collection[i]))\n",
    "test_dff = to_test_df(test_vocabulary_collection , dictionary)\n",
    "for i in range(len(test_df)) :\n",
    "    test_dff[i] = test_dff[i].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 5, 5, 1, 5, 5, 2, 2, 5, 4, 3, 4, 1, 4, 5, 4, 4, 2, 3, 4, 2, 4, 5, 2, 5, 1, 3, 1, 3, 1, 3, 3, 3, 4, 3, 1, 1, 2, 2, 5, 1, 5, 3, 3, 4, 1, 3, 5, 5, 3, 3, 1, 5, 3, 4, 3, 4, 1, 5, 5, 1, 5, 4, 1, 3, 5, 2, 5, 4, 5, 5, 2, 1, 5, 2, 2, 2, 4, 3, 5, 1, 1, 1, 4, 1, 2, 1, 1, 4, 2, 1, 3, 5, 2, 2, 1, 4, 1, 3, 1, 5, 4, 2, 4, 3, 5, 2, 4, 1, 3, 4, 1, 3, 1, 4, 3, 1, 1, 2, 3, 2, 1, 1, 1, 1, 5, 3, 4, 5, 4, 1, 4, 2, 4, 1, 4, 4, 4, 5, 1, 3, 4, 2, 5, 1, 4, 4, 2, 1, 3, 1, 2, 1, 2, 1, 1, 3, 2, 1, 3, 1, 1, 5, 4, 2, 5, 1, 1, 1, 1, 4, 1, 3, 3, 1, 1, 4, 3, 2, 5, 1, 4, 1, 1, 1, 1, 1, 1, 3, 4, 5, 5, 5, 4, 1, 4, 4, 3, 1, 4, 1, 2, 1, 1, 5, 4, 4, 4, 4, 1, 1, 4, 1, 3, 1, 1, 4, 5, 3, 1, 4, 3, 1, 4, 4, 3, 2, 1, 4, 1, 1, 5, 1, 3, 2, 5, 5, 1, 5, 1, 3, 1, 3, 3, 5, 1, 4, 1, 1, 1, 5, 2, 3, 1, 5, 4, 2, 5, 1, 3, 1, 3, 4, 3, 4, 2, 4, 2, 4, 1, 4, 2, 4, 1, 1, 3, 1, 4, 1, 1, 4, 4, 1, 3, 4, 5, 4, 2, 4, 4, 1, 1, 1, 4, 4, 3, 1, 4, 4, 1, 4, 5, 5, 3, 1, 1, 2, 2, 1, 5, 2, 2, 5, 2, 2, 2, 4, 4, 3, 3, 3, 3, 5, 1, 5, 1, 1, 1, 1, 5, 1, 1, 2, 2, 5, 4, 5, 5, 4, 5, 2, 2, 3, 4, 5, 2, 2, 3, 5, 5, 4, 3, 4, 5, 2, 4, 2, 3, 3, 1, 5, 5, 4, 5, 4, 1, 3, 2, 2, 2, 5, 1, 5, 1, 3, 3, 5, 3, 1, 5, 5, 1, 2, 1, 3, 5, 2, 1, 1, 2, 2, 3, 1, 5, 5, 4, 4, 1, 3, 5, 5, 1, 2, 2, 2, 3, 1, 4, 4, 3, 5, 4, 5, 2, 3, 4, 3, 5, 2, 4, 4, 4, 3, 1, 1, 3, 2, 4, 3, 1, 3, 1, 4, 4, 4, 1, 5, 4, 1, 5, 1, 4, 2, 2, 1, 5, 5, 5, 3, 1, 5, 2, 5, 1, 5, 2, 1, 1, 3, 2, 3, 3, 5, 4, 1, 5, 1, 5, 3, 2, 5, 5, 4, 1, 5, 2, 4, 1, 1, 1, 5, 2, 4, 3, 4, 1, 1, 1, 1, 1, 3, 4, 4, 1, 3, 4, 4, 1, 4, 4, 1, 1, 4, 1, 5, 1, 1, 5, 4, 4, 4, 5, 4, 5, 5, 2, 1, 1, 1, 1, 5, 4, 4, 4, 2, 4, 4, 5, 1, 2, 1, 5, 3, 4, 4, 4, 2, 1, 2, 1, 5, 3, 1, 2, 5, 4, 4, 4, 1, 2, 1, 1, 5, 1, 5, 5, 2, 5, 4, 4, 3, 5, 4, 2, 5, 5, 4, 2, 2, 4, 1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 2, 4, 1, 4, 1, 1, 3, 2, 1, 1, 2, 4, 2, 5, 5, 5, 1, 1, 3, 5, 5, 4, 5, 4, 4, 2, 5, 5, 4, 4, 2, 1, 2, 4, 4, 4, 4, 5, 4, 1, 3, 1, 4, 4, 1, 4, 1, 2, 4, 2, 5, 5, 1, 1, 1, 5, 5, 3, 4, 3, 2, 4, 5, 4, 2, 2, 4, 3, 3, 4, 1, 4, 5, 4, 1, 1, 1, 5, 4, 4, 4, 4, 5, 3, 2, 4, 1, 1, 5, 3, 4, 5, 5, 5, 5, 3, 5, 5, 1, 3, 1, 1, 4, 3, 1, 4, 4, 2, 5, 1, 3, 1, 1, 5, 1, 5, 3, 5, 5, 1, 4, 1, 5, 5, 3, 2, 3, 2, 1, 5, 1, 4, 4, 3, 1, 1, 1, 4, 3, 1, 5, 5, 1, 2, 1, 5, 4, 2, 1, 2, 4, 5, 1, 5, 5, 5, 3, 5, 5, 3, 4, 5, 5, 5, 3, 4, 5, 1, 4, 4, 5, 5, 4, 4, 2, 5, 1, 5, 5, 1, 5, 5, 1, 4, 5, 3, 5, 3, 3, 5, 3, 5, 1, 2, 5, 5, 5, 4, 5, 5, 5, 3, 2, 5, 4, 2, 1, 1, 5, 1, 2, 1, 3, 3, 2, 1, 2, 5, 4, 4, 4, 3, 1, 5, 2, 5, 5, 2, 5, 1, 5, 1, 2, 2, 2, 5, 5, 5, 4, 5, 2, 1, 3, 1, 1, 2, 4, 1, 5, 3, 2, 3, 5, 1, 2, 5, 5, 3, 5, 4, 4, 5, 1, 2, 2, 3, 1, 5, 1, 2, 5, 5, 5, 2, 4, 5, 5, 3, 4, 2, 5, 1, 4, 3, 1, 2, 4, 5, 2, 5, 2, 5, 5, 1, 1, 4, 3, 1, 2, 3, 5, 1, 1, 5, 1, 5, 5, 5, 5, 1, 5, 4, 4, 5, 5, 5, 5, 2, 3, 4, 4, 4, 3, 1, 5, 5, 4, 1, 5, 4, 5, 5, 1, 4, 5, 5, 5, 1, 2, 3, 3, 3, 2, 3, 3, 4, 2, 3, 3, 4, 5, 5, 1, 3, 5, 5, 5, 1, 5, 5, 1, 5, 4, 5, 1, 5, 2, 5, 1, 1, 3, 5, 1, 5, 5, 5, 3, 1, 5, 2, 5, 2, 1, 5, 5, 1, 2, 5, 5, 5, 4, 1, 3, 2, 3, 5, 2, 3, 4, 4, 5, 5, 4, 2, 5, 1, 4, 2, 1, 5, 5, 5, 1, 3, 2, 1, 4, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "answer = []\n",
    "for d in test_dff :\n",
    "    class_index = 0\n",
    "    clist = []\n",
    "    for c in class_df :\n",
    "        if class_index == 0 :\n",
    "            p = math.log(2368 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 1 :\n",
    "            p = math.log(1024 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 2 :\n",
    "            p = math.log(231 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 3 :\n",
    "            p = math.log(1987 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 4 :\n",
    "            p = math.log(129 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        else :\n",
    "            print(\"wrong class index\")\n",
    "        class_index = class_index + 1\n",
    "        for index , row in d.iterrows() :\n",
    "            t = row['term']\n",
    "            if (t in c.values) and (t in dictionary.values) :\n",
    "                class_df_index = c[c['term'] == t].index.tolist()\n",
    "                index = class_df_index[0]\n",
    "#                 print(index)\n",
    "                prob = c.iloc[index]['probability']\n",
    "#                 print(prob)\n",
    "                p = p + prob * row['tf']\n",
    "        clist.append(p)  \n",
    "    maxi = -99999999\n",
    "    maxIndex = -1\n",
    "    for i in range(len(clist)):\n",
    "        if clist[i] > maxi:\n",
    "            maxi = clist[i]\n",
    "            maxIndex = i + 1\n",
    "    answer.append(maxIndex)\n",
    "print(answer)\n",
    "answer = [x - 1 for x in answer]\n",
    "index = list(map(int , test_list))\n",
    "output = pd.DataFrame(list(zip(test_list , answer)) , columns =['Id', 'Value'])\n",
    "output.to_csv('pochun_testing/result.csv' , index = None , sep = ',')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - 4 分別代表 'euclid' 'thaumiel' 'keter' 'safe' 'neutralized'\n",
    "以上印出來的結果應該全部 -1(輸出的csv有減))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for c in class_df :\n",
    "    c.to_csv('pochun_testi有檢/class' + str(count) + '.txt' , index = None , sep = ' ')\n",
    "    count  = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for t in test_dff :\n",
    "    t.to_csv('pochun_testing/test' + str(count) + '.txt' , index = None , sep = ' ')\n",
    "    count  = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.to_csv('pochun_testing/dic500' + str(count) + '.txt' , index = None , sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['object_class'] = test_df['object_class'].replace(\"euclid\", 0)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"thaumiel\", 1)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"keter\", 2)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"safe\", 3)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"neutralized\", 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(1000) :\n",
    "    if(test_df.at[i , 'object_class'] == answer[i]) :\n",
    "        count = count + 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('pochun_testing/test_df.csv' , index = None , sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
