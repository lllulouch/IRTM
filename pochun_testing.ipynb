{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "document_frequency = []\n",
    "with open('test/dictionary.txt') as txt:\n",
    "    for line in txt:\n",
    "        row = line.split()\n",
    "        data.append(row[ : -1])\n",
    "        document_frequency.append(row[-2])\n",
    "document_frequency = np.asarray(document_frequency)        \n",
    "document_frequency = document_frequency.astype(np.int)\n",
    "document_frequency = np.insert(document_frequency , 0 , 0 , axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5739\n",
    "N_arr = np.ones(len(document_frequency)) * N\n",
    "data = []\n",
    "term_index = []\n",
    "term_frequency = []\n",
    "doc_tf = []\n",
    "doc_idf = []\n",
    "doc_tfidf = []\n",
    "norm_doc_tfidf = []\n",
    "for i in range(N) :\n",
    "    term_index.append([])\n",
    "    term_frequency.append([])\n",
    "    doc_tf.append([])\n",
    "    doc_idf.append([])\n",
    "    doc_tfidf.append([])\n",
    "    norm_doc_tfidf.append([])\n",
    "    doc_tf[i] = np.zeros(len(document_frequency), dtype = int)\n",
    "    with open('test/binary_vector/doc' + str(i) + '.txt') as txt :\n",
    "        for line in txt:\n",
    "            row = line.split()\n",
    "            data.append(row[ : -1])\n",
    "            term_index[i].append(row[0])\n",
    "            term_frequency[i].append(row[-1])\n",
    "    term_index[i].pop(0)\n",
    "    term_frequency[i].pop(0)\n",
    "    j = 0\n",
    "    for index in term_index[i] :\n",
    "        doc_tf[i][int(index)] = term_frequency[i][j]\n",
    "        j = j + 1\n",
    "    doc_idf[i] = np.log10(np.divide(N_arr , document_frequency))\n",
    "    doc_tfidf[i] = np.multiply(doc_idf[i] , doc_tf[i])\n",
    "    norm_doc_tfidf[i] = doc_tfidf[i] / np.linalg.norm(doc_tfidf[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_doc_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('norm_tfidf.txt' , 'w') as f :\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerows(norm_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_idf = np.log10(np.divide(N_arr , document_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i \n",
    "    i = 0\n",
    "    for index in term_index :\n",
    "        doc_tf[int(index)] = term_frequency[i]\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tf = np.zeros(len(document_frequency) , dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tfidf = np.multiply(doc_idf , doc_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_doc_tfidf = doc_tfidf / np.linalg.norm(doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "d.append([])\n",
    "print(type(d[0]))\n",
    "d[0].append(1)\n",
    "print(type(d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(term_frequency[5738])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拍謝頗雜亂\n",
    "by pochun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/irtm.csv')  \n",
    "count_row = df.shape[0] \n",
    "count_col = df.shape[1]\n",
    "print(count_row)\n",
    "print(count_col)\n",
    "print(df.columns)\n",
    "print(df['object_class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max rating \\n\" , df.loc[df['rating'].idxmax()] , \"\\n\")\n",
    "print(\"Min rating \\n\" , df.loc[df['rating'].idxmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.plot.hist(bins = 100 , xlim = (-100 , 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('object_class').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [i for i in range(count_row)]\n",
    "df['index'] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training validation testing 的切法還須調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[4500 : count_row] \n",
    "val_df = df.iloc[3000 : 4000]\n",
    "test_df = df.iloc[0 : 1000]\n",
    "train_df = train_df.reset_index()\n",
    "val_df = val_df.reset_index()\n",
    "test_df = test_df.reset_index()\n",
    "train_df = train_df.drop(columns = ['level_0'])\n",
    "val_df = val_df.drop(columns = ['level_0'])\n",
    "test_df = test_df.drop(columns = ['level_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_test_df = test_df.groupby(\"object_class\")\n",
    "euclid_test_df = class_test_df.get_group(\"euclid\")\n",
    "thaumiel_test_df = class_test_df.get_group(\"thaumiel\")\n",
    "keter_test_df = class_test_df.get_group(\"keter\")\n",
    "safe_test_df = class_test_df.get_group(\"safe\")\n",
    "# 照此切法 testing 沒有 neutralized QQ\n",
    "# neutralized_test_df = class_test_df.get_group(\"neutralized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train_df = train_df.groupby(\"object_class\")\n",
    "euclid_train_df = class_train_df.get_group(\"euclid\")\n",
    "thaumiel_train_df = class_train_df.get_group(\"thaumiel\")\n",
    "keter_train_df = class_train_df.get_group(\"keter\")\n",
    "safe_train_df = class_train_df.get_group(\"safe\")\n",
    "neutralized_train_df = class_train_df.get_group(\"neutralized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_index_list_train = euclid_train_df['index'].tolist()\n",
    "t_index_list_train = thaumiel_train_df['index'].tolist()\n",
    "k_index_list_train = keter_train_df['index'].tolist()\n",
    "s_index_list_train = safe_train_df['index'].tolist()\n",
    "n_index_list_train = neutralized_train_df['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是我的 PA3 經過些微調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def term_extraction (txt):\n",
    "    txt = str(txt) \n",
    "    punctuation = [\"\\n\" , \"``\" , '$' , ', ' , '. ' , '; ' , '.' , '_' , ':' , \"''\" , '\\\"' , '\\'' , '\\\"' , '(' , ')', '!' , '?' , '‘' , '*' , '&' , '`' , '\\\\']\n",
    "    for mark in punctuation:\n",
    "        txt = txt.replace(mark , ' ')\n",
    "    txt = txt.replace(',' , '')\n",
    "    txt = txt.replace(\"\\'s \" , \" is \")\n",
    "    txt = txt.replace(\"-\" , \" \")\n",
    "    split_txt = []\n",
    "    temp = ''\n",
    "    for char in txt:\n",
    "        if char == ' ':\n",
    "            split_txt.append(temp)\n",
    "            temp = ''\n",
    "        else:\n",
    "            temp += char\n",
    "    if temp:\n",
    "        split_txt.append(temp)\n",
    "    token = []\n",
    "    for s in split_txt:\n",
    "        token.append(s.lower())\n",
    "    ps_token = []\n",
    "    for word in token:\n",
    "        ps_token.append(ps.stem(word))\n",
    "    stop_word = [\"\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    final = []\n",
    "    for word in ps_token:\n",
    "        appear = False\n",
    "        for stop in stop_word:\n",
    "            if word == stop:\n",
    "                appear = True\n",
    "        if not appear and word.isalpha():\n",
    "            final.append(word)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dictionary (vocabulary_collection) :\n",
    "    dictionary = pd.DataFrame(columns = ['term' , 'df' , 'cf'])\n",
    "    df = 0\n",
    "    cf = 0\n",
    "    for d in vocabulary_collection:\n",
    "        for t in d:\n",
    "            if t not in dictionary.values:        \n",
    "                for d2 in vocabulary_collection :\n",
    "                    cf = cf + d2.count(t)\n",
    "                    if t in d2:\n",
    "                        df = df + 1\n",
    "                dictionary.loc[-1] = [t , df , cf]  \n",
    "                dictionary.index = dictionary.index + 1\n",
    "            df = 0\n",
    "            cf = 0\n",
    "    dictionary.sort_values(by = ['term'] , inplace = True , ascending = True)\n",
    "    dictionary = dictionary.reset_index(drop = True)\n",
    "    dictionary.to_csv('pochun_testing/dictionary.txt' , index = None , sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class_dictionary (train_df , dictionary) :\n",
    "    class_df = []\n",
    "    for i in range (5) :\n",
    "        if (i == 0) :\n",
    "            cdocument_list = e_index_list_train\n",
    "        elif (i == 1) :\n",
    "            cdocument_list = t_index_list_train\n",
    "        elif (i == 2) :\n",
    "            cdocument_list = k_index_list_train\n",
    "        elif (i == 3) :\n",
    "            cdocument_list = s_index_list_train\n",
    "        elif (i == 4) :\n",
    "            cdocument_list = n_index_list_train\n",
    "        else :\n",
    "            print(\"wrong_class_index\")\n",
    "\n",
    "        class_collection = ''\n",
    "        for docId in cdocument_list :     \n",
    "            class_df_index = train_df[train_df['index'] == docId].index.tolist()\n",
    "            index = class_df_index[0]\n",
    "            string = train_df.iloc[index]['content']  \n",
    "            class_collection = class_collection + string\n",
    "        class_vocabulary = term_extraction(class_collection)\n",
    "        c = pd.DataFrame(columns = ['term' , 'classf'])\n",
    "        for t in dictionary['term'] :\n",
    "            classf = class_vocabulary.count(t)\n",
    "            c.loc[-1] = [t , classf]\n",
    "            c.index = c.index + 1    \n",
    "        class_df.append(c)\n",
    "    for c in class_df :\n",
    "        prob = []\n",
    "        for index , row in c.iterrows() :        \n",
    "            prob.append(math.log((row['classf'] + 1) / (c['classf'].sum() + 500)))\n",
    "        c['probability'] = prob\n",
    "    return class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class_vocabulary (train_df) :\n",
    "    class_vocabulary = [] \n",
    "    \n",
    "    for i in range(5):\n",
    "        class_vocabulary.append([])\n",
    "\n",
    "    for i in range (5) :\n",
    "        if (i == 0) :\n",
    "            cdocument_list = e_index_list_train\n",
    "        elif (i == 1) :\n",
    "            cdocument_list = t_index_list_train\n",
    "        elif (i == 2) :\n",
    "            cdocument_list = k_index_list_train\n",
    "        elif (i == 3) :\n",
    "            cdocument_list = s_index_list_train\n",
    "        elif (i == 4) :\n",
    "            cdocument_list = n_index_list_train\n",
    "        else :\n",
    "            print(\"wrong_class_index\")\n",
    "        \n",
    "        for docId in cdocument_list :\n",
    "#             print(docId)\n",
    "            class_df_index = train_df[train_df['index'] == docId].index.tolist()\n",
    "#             print(class_df_index)\n",
    "            index = class_df_index[0]\n",
    "            string = train_df.iloc[index]['content'] \n",
    "            class_vocabulary[i].append(term_extraction(string)) \n",
    "    return class_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_test_df (test_vocabulary_collection , dictionary) :\n",
    "    test_df = []\n",
    "    for d in test_vocabulary_collection :\n",
    "        tdoc = pd.DataFrame(columns = ['term' , 'tf'])\n",
    "        for t in d :\n",
    "            tf = 0\n",
    "            if (t in dictionary.values) and (t not in tdoc.values) :\n",
    "                tf = d.count(t)\n",
    "                tdoc.loc[-1] = [t , tf]\n",
    "                tdoc.index = tdoc.index + 1    \n",
    "        test_df.append(tdoc)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(\"pochun_testing\")\n",
    "all_list = []\n",
    "for i in range(count_row) :\n",
    "    all_list.append(str(i))\n",
    "train_list = [i for i in range(4500 , count_row)]\n",
    "val_list = [i for i in range(3000 , 4000)]\n",
    "test_list = [i for i in range(0 , 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "for docId in train_list : \n",
    "    class_df_index = df[df['index'] == docId].index.tolist()\n",
    "    index = class_df_index[0]\n",
    "    string = df.iloc[index]['content'] \n",
    "    collection.append(string)\n",
    "vocabulary_collection = []\n",
    "for i in range(len(collection)):\n",
    "    vocabulary_collection.append(term_extraction(collection[i]))\n",
    "to_dictionary(vocabulary_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.read_csv('pochun_testing/dictionary.txt' , sep = \" \")\n",
    "zeros = [0] * len(dictionary)\n",
    "dictionary['chi'] = zeros\n",
    "class_vocabulary = to_class_vocabulary(train_df)\n",
    "chi_table = []\n",
    "\n",
    "for term_index in range(len(dictionary)) :\n",
    "    chi_n_2 = pd.DataFrame()\n",
    "    zeros13 = [-1] * 13\n",
    "    chi_n_2['present'] = zeros13\n",
    "    chi_n_2['absent'] = zeros13\n",
    "    term = dictionary.iloc[term_index]['term']\n",
    "    for class_index in range(len(class_vocabulary)) :\n",
    "        present = 0\n",
    "        for class_doc_index in range(len(class_vocabulary[class_index])) :\n",
    "            \n",
    "            if term in class_vocabulary[class_index][class_doc_index] :\n",
    "                present = present + 1\n",
    "        chi_n_2['present'][class_index] = present\n",
    "        absent = 15 - present\n",
    "        chi_n_2['absent'][class_index] = absent\n",
    "    chi_score = 0\n",
    "    for index , row in chi_n_2.iterrows() :\n",
    "        expect = (15 / (13 * 15)) * (chi_n_2['present'].sum() / (13 * 15)) * (13 * 15)\n",
    "        n = row['present']\n",
    "        chi_score = chi_score + ((n - expect)**2 / expect)\n",
    "        expect = (15 / (13 * 15)) * (chi_n_2['absent'].sum() / (13 * 15)) * (13 * 15)\n",
    "        n = row['absent']\n",
    "        chi_score = chi_score + ((n - expect)**2 / expect)\n",
    "    dictionary['chi'][term_index] = chi_score\n",
    "dictionary = dictionary.sort_values(by = ['chi'] , ascending = False)\n",
    "dictionary = dictionary.sort_values(by = ['chi'] , ascending = False)\n",
    "dictionary = dictionary.reset_index(drop = True)\n",
    "dictionary = dictionary.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chi square == inf 應該有錯誤QQ(暫時查無原因QQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = to_class_dictionary(train_df , dictionary)\n",
    "for i in range(len(class_df)) :\n",
    "    class_df[i] = class_df[i].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection = []\n",
    "for docId in test_list:        \n",
    "    class_df_index = test_df[test_df['index'] == docId].index.tolist()\n",
    "    index = class_df_index[0]\n",
    "    string = test_df.iloc[index]['content'] \n",
    "    test_collection.append(string)\n",
    "test_vocabulary_collection = []\n",
    "for i in range(len(test_collection)):\n",
    "    test_vocabulary_collection.append(term_extraction(test_collection[i]))\n",
    "test_dff = to_test_df(test_vocabulary_collection , dictionary)\n",
    "for i in range(len(test_df)) :\n",
    "    test_dff[i] = test_dff[i].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "for d in test_dff :\n",
    "    class_index = 0\n",
    "    clist = []\n",
    "    for c in class_df :\n",
    "        if class_index == 0 :\n",
    "            p = math.log(2368 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 1 :\n",
    "            p = math.log(1024 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 2 :\n",
    "            p = math.log(231 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 3 :\n",
    "            p = math.log(1987 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        elif class_index == 4 :\n",
    "            p = math.log(129 / (2368 + 1024 + 231 + 1987 + 129))\n",
    "        else :\n",
    "            print(\"wrong class index\")\n",
    "        class_index = class_index + 1\n",
    "        for index , row in d.iterrows() :\n",
    "            t = row['term']\n",
    "            if (t in c.values) and (t in dictionary.values) :\n",
    "                class_df_index = c[c['term'] == t].index.tolist()\n",
    "                index = class_df_index[0]\n",
    "#                 print(index)\n",
    "                prob = c.iloc[index]['probability']\n",
    "#                 print(prob)\n",
    "                p = p + prob * row['tf']\n",
    "        clist.append(p)  \n",
    "    maxi = -99999999\n",
    "    maxIndex = -1\n",
    "    for i in range(len(clist)):\n",
    "        if clist[i] > maxi:\n",
    "            maxi = clist[i]\n",
    "            maxIndex = i + 1\n",
    "    answer.append(maxIndex)\n",
    "print(answer)\n",
    "answer = [x - 1 for x in answer]\n",
    "index = list(map(int , test_list))\n",
    "output = pd.DataFrame(list(zip(test_list , answer)) , columns =['Id', 'Value'])\n",
    "output.to_csv('pochun_testing/result.csv' , index = None , sep = ',')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - 4 分別代表 'euclid' 'thaumiel' 'keter' 'safe' 'neutralized'\n",
    "以上印出來的結果應該全部 -1(輸出的csv有減))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for c in class_df :\n",
    "    c.to_csv('pochun_testi有檢/class' + str(count) + '.txt' , index = None , sep = ' ')\n",
    "    count  = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for t in test_dff :\n",
    "    t.to_csv('pochun_testing/test' + str(count) + '.txt' , index = None , sep = ' ')\n",
    "    count  = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.to_csv('pochun_testing/dic500' + str(count) + '.txt' , index = None , sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['object_class'] = test_df['object_class'].replace(\"euclid\", 0)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"thaumiel\", 1)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"keter\", 2)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"safe\", 3)\n",
    "test_df['object_class'] = test_df['object_class'].replace(\"neutralized\", 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(1000) :\n",
    "    if(test_df.at[i , 'object_class'] == answer[i]) :\n",
    "        count = count + 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('pochun_testing/test_df.csv' , index = None , sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
